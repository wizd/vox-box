services:
  # 语音识别 (Speech-to-Text) - 支持中文
  # faster-whisper-large-v3: 最佳多语言模型，中文识别效果优秀
  vox-stt:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: vox-stt
    ports:
      - "8080:80"
    volumes:
      - vox-stt-data:/data
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    command:
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "80"
      - "--device"
      - "cuda:0"
      - "--huggingface-repo-id"
      - "Systran/faster-whisper-large-v3"
      - "--data-dir"
      - "/data"
    restart: unless-stopped

  # 语音合成 (Text-to-Speech) - 支持中文
  # CosyVoice-300M-SFT: 高质量中文语音合成模型（已验证 Linux 支持）
  vox-tts:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: vox-tts
    ports:
      - "8082:80"
    volumes:
      - vox-tts-data:/data
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    command:
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "80"
      - "--device"
      - "cuda:0"
      - "--huggingface-repo-id"
      - "FunAudioLLM/CosyVoice-300M-SFT"
      - "--data-dir"
      - "/data"
    restart: unless-stopped

  # Qwen3-TTS - 高性能低延迟中文语音合成 (vLLM-Omni)
  # 独立服务，与 CosyVoice 并存，API 兼容 OpenAI /v1/audio/speech
  qwen3-tts:
    image: vllm/vllm-omni:v0.14.0
    container_name: qwen3-tts
    ports:
      - "8083:8000"
    volumes:
      - qwen3-tts-cache:/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    ipc: host
    entrypoint: ["vllm", "serve", "--omni"]
    command:
      - "Qwen/Qwen3-TTS-12Hz-1.7B-CustomVoice"
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "8000"
      - "--gpu-memory-utilization"
      - "0.5"
      - "--trust-remote-code"
      - "--enforce-eager"
    restart: unless-stopped

  # CosyVoice3 - 高性能流式中文语音合成 (Fun-CosyVoice3-0.5B)
  # OpenAI 兼容 API, 流式 PCM, ~1.2s TTFB
  # 基于 neosun/cosyvoice:v3.4.0 升级 PyTorch 以支持 RTX 5090 Blackwell
  cosyvoice3:
    build:
      context: .
      dockerfile: Dockerfile.cosyvoice3
    container_name: cosyvoice3
    ports:
      - "8188:8188"
    volumes:
      - cosyvoice3-voices:/data/voices
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ["0"]
              capabilities: [gpu]
    restart: unless-stopped

  # CosyVoice2 TRT-LLM - 极致低延迟流式 TTS (Triton + TensorRT-LLM)
  # 流式 TTFB ~190ms, RTF < 0.15, gRPC streaming
  # 首次启动需下载模型 + 编译 TRT 引擎，约 30-60 分钟
  cosyvoice2-trt:
    image: soar97/triton-cosyvoice:25.06
    container_name: cosyvoice2-trt
    ipc: host
    shm_size: '2gb'
    ports:
      - "8000:8000"   # Triton HTTP
      - "8001:8001"   # Triton gRPC (流式推荐)
      - "8002:8002"   # Triton metrics
    volumes:
      - cosyvoice2-trt-data:/workspace/persist
      - ./scripts/patch_cosyvoice2_token2wav.py:/workspace/patches/patch_cosyvoice2_token2wav.py:ro
      - ./scripts/patch_cosyvoice2_llm_thread.py:/workspace/patches/patch_cosyvoice2_llm_thread.py:ro
    environment:
      - PYTHONIOENCODING=utf-8
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]
    command: >
      /bin/bash -c "
        set -e &&
        PERSIST=/workspace/persist &&

        echo '=== Step 1: Clone CosyVoice repo ===' &&
        if [ ! -d $$PERSIST/CosyVoice ]; then
          git clone --recursive https://github.com/FunAudioLLM/CosyVoice.git $$PERSIST/CosyVoice ;
        fi &&
        ln -sfn $$PERSIST/CosyVoice /workspace/CosyVoice &&

        echo '=== Step 1.5: Patch token2wav template for short mel ===' &&
        python3 /workspace/patches/patch_cosyvoice2_token2wav.py --target $$PERSIST/CosyVoice/runtime/triton_trtllm/model_repo/token2wav/1/model.py &&

        cd /workspace/CosyVoice/runtime/triton_trtllm &&

        echo '=== Step 1.6: Fix run.sh bugs ===' &&
        sed -i 's|fill_template.py -i /tensorrt_llm/|fill_template.py -i $$model_repo/tensorrt_llm/|' run.sh &&
        sed -i '/tensorrt_llm.*config.pbtxt/s|triton_max_batch_size:,|triton_max_batch_size:16,|' run.sh &&
        sed -i '/tensorrt_llm.*config.pbtxt/s|decoupled_mode:,|decoupled_mode:True,|' run.sh &&
        sed -i '/tensorrt_llm.*config.pbtxt/s|engine_dir:,|engine_dir:./trt_engines_bfloat16,|' run.sh &&
        sed -i '/tensorrt_llm.*config.pbtxt/s|max_queue_delay_microseconds:,|max_queue_delay_microseconds:0,|' run.sh &&
        sed -i '/tensorrt_llm.*config.pbtxt/s|max_queue_delay_microseconds:$$|max_queue_delay_microseconds:0|' run.sh &&

        echo '=== Step 2: Download models from HuggingFace ===' &&
        if [ ! -f cosyvoice2_llm/model.safetensors ]; then
          huggingface-cli download --local-dir cosyvoice2_llm yuekai/cosyvoice2_llm ;
        fi &&
        if [ ! -f CosyVoice2-0.5B/cosyvoice2.yaml ]; then
          huggingface-cli download --local-dir CosyVoice2-0.5B FunAudioLLM/CosyVoice2-0.5B ;
        fi &&
        if [ ! -f CosyVoice2-0.5B/spk2info.pt ]; then
          wget -q https://raw.githubusercontent.com/qi-hua/async_cosyvoice/main/CosyVoice2-0.5B/spk2info.pt -O CosyVoice2-0.5B/spk2info.pt ;
        fi &&

        echo '=== Step 3: Build TRT engines ===' &&
        if [ ! -f trt_engines_bfloat16/rank0.engine ]; then
          bash run.sh 1 1 ;
        fi &&

        echo '=== Step 4: Create model repo ===' &&
        bash run.sh 2 2 &&

        echo '=== Step 4.5: Patch cosyvoice2 LLM thread error handling ===' &&
        python3 /workspace/patches/patch_cosyvoice2_llm_thread.py --target /workspace/CosyVoice/runtime/triton_trtllm/model_repo_cosyvoice2/cosyvoice2/1/model.py &&

        echo '=== Step 4.6: Tune TRT-LLM KV cache + scheduler ===' &&
        sed -i '/key: \"max_tokens_in_paged_kv_cache\"/{n;n;s|string_value: \".*\"|string_value: \"8192\"|;}' /workspace/CosyVoice/runtime/triton_trtllm/model_repo_cosyvoice2/tensorrt_llm/config.pbtxt &&
        sed -i '/key: \"kv_cache_free_gpu_mem_fraction\"/{n;n;s|string_value: \".*\"|string_value: \"0.5\"|;}' /workspace/CosyVoice/runtime/triton_trtllm/model_repo_cosyvoice2/tensorrt_llm/config.pbtxt &&
        sed -i '/key: \"batch_scheduler_policy\"/{n;n;s|string_value: \".*\"|string_value: \"max_utilization\"|;}' /workspace/CosyVoice/runtime/triton_trtllm/model_repo_cosyvoice2/tensorrt_llm/config.pbtxt &&
        sed -i 's/default_queue_policy: { max_queue_size: 0 }/default_queue_policy: { max_queue_size: 32 }/' /workspace/CosyVoice/runtime/triton_trtllm/model_repo_cosyvoice2/tensorrt_llm/config.pbtxt &&

        echo '=== Step 5: Start Triton ===' &&
        bash run.sh 3 3
      "
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://127.0.0.1:8000/v2/health/ready > /dev/null || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 120s
    logging:
      driver: json-file
      options:
        max-size: "100m"
        max-file: "3"
    restart: unless-stopped

  # CosyVoice2 OpenAI-Compatible HTTP Bridge
  # 将 /v1/audio/speech API 转发到 Triton gRPC，返回流式 WAV/PCM
  cosyvoice2-bridge:
    build:
      context: .
      dockerfile: Dockerfile.cosyvoice2-bridge
    container_name: cosyvoice2-bridge
    ports:
      - "9880:9880"
    environment:
      - PUNCT_BUFFER_ENABLED=false
      - PUNCT_MIN_CHARS=12
      - PUNCT_MAX_CHARS=80
      - PUNCT_MAX_WAIT_MS=400
      - OUTPUT_JITTER_BUFFER_MS=650
      - NEXT_CHUNK_TIMEOUT_S=3
      - SESSION_SERIALIZE_ENABLED=true
      - SESSION_LOCK_TIMEOUT_S=120
    depends_on:
      cosyvoice2-trt:
        condition: service_healthy
    restart: unless-stopped

volumes:
  vox-stt-data:
  vox-tts-data:
  qwen3-tts-cache:
  cosyvoice3-voices:
  cosyvoice2-trt-data: